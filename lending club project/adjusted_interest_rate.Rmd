---
title: "Lending Club Project -- Predict Risk Adjusted Interest Rate"
author: "Yi Li"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r}
loan = read.csv('loan.csv', stringsAsFactors = F)
loanT = loan

#length(which(is.na(loan$annual_inc)))

num.NA = sort(sapply(loan, function(x){sum(is.na(x))}), decreasing = T)
num.NA

## remove feature (>80%), remove data,
remain.col = names(num.NA)[which(num.NA <= 0.8*dim(loan)[1])]
length(remain.col)
loan = loan[, remain.col]


# Building model
# 1. First think about what features could be included in the model
# i.e., what features would be available during model building. Work example.
# e.g., loan payment features will not be available when deciding interest rate.

# 2. Second think about what features should not be included in the model
# i.e., Remove features using intuition, 
#       Remove features with unique value per row or no variance. 
#       Remove redundant features
# e.g., id, member.id

## remove these features in the model
num.value = sapply(loan, function(x){return(length(unique(x)))})
which(num.value == 1)
which(num.value == dim(loan)[1])

# DTI (debt to income ratio)
summary(loanT$dti_joint)
with(subset(loanT, is.na(dti_joint)), table(application_type))
loan$dti = ifelse(!is.na(loanT$dti_joint), loanT$dti_joint, loanT$dti)
loan$annual_inc = ifelse(!is.na(loanT$annual_inc_joint), loanT$annual_inc_joint, loanT$annual_inc)


# 3. Then think about if we need process existing features.
# For category variables
# if category variables has too many levels, Take addr_state for example.
# Disadvantage of using such variable as it is.
# Find similar levels and collapse them, how to find similar level though?
# Simple example, collapse by definitoin
table(loan$home_ownership)
loan$home_ownership = ifelse(loan$home_ownership %in% c('ANY', 'NONE', 'OTHER'), 'OTHER',
                             loan$home_ownership)

# More complicated example, collapse by calculation
int_state = by(loan, loan$addr_state, function(x){
  return(mean(x$int_rate))})

loan$state_mean_int = 
  ifelse(loan$addr_state %in% names(int_state)[which(int_state<=quantile(int_state,0.25))],'low',
         ifelse(loan$addr_state %in% names(int_state)[which(int_state<=quantile(int_state,0.5))],'lowmedium',
                ifelse(loan$addr_state %in% names(int_state)[which(int_state<=quantile(int_state,0.75))],'mediumhigh', 
                       'high')))

table(loan$state_mean_int)

## do similar collapse for the feature purpose
table(loan$purpose)
int_purpuse = by(loan, loan$purpose, function(x){
  return(mean(x$int_rate))})

loan$purpose_mean_int = 
  ifelse(loan$purpose %in% names(int_purpuse)[which(int_purpuse<=quantile(int_purpuse,0.25))],'low',
         ifelse(loan$purpose %in% names(int_purpuse)[which(int_purpuse<=quantile(int_purpuse,0.5))],'lowmedium',
                ifelse(loan$purpose %in% names(int_purpuse)[which(int_purpuse<=quantile(int_purpuse,0.75))],'mediumhigh', 
                       'high')))

table(loan$purpose_mean_int)

# 4. Moreover, what new features could be engineered.
library(zoo)
loan$issue_d_1 <- as.Date(as.yearmon(loan$issue_d, "%b-%Y"))
loan$issue_year <- as.character(format(loan$issue_d_1, "%Y"))
loan$issue_mon <- as.character(format(loan$issue_d_1, "%m"))
int.by.year <- by(loan, loan$issue_year, function(x){return(mean(x$int_rate))})
plot(int.by.year)
int.by.mon <- by(loan, loan$issue_mon, function(x){return(mean(x$int_rate))})
plot(int.by.mon)

# before splitting
loan$log_annual_inc = log(loan$annual_inc+1)
loan.sub = loan[, c('int_rate', 'state_mean_int', 'home_ownership', 'log_annual_inc', 'dti',
                      'term', 'loan_amnt', 'total_acc', 'tot_cur_bal', #'open_acc',
                      'purpose_mean_int', 'installment', 'verification_status')]
num.NA <- sort(sapply(loan.sub, function(x) { sum(is.na(x))} ), decreasing = TRUE)
num.NA
# use median to impute missing value
loan.sub$tot_cur_bal[which(is.na(loan.sub$tot_cur_bal))] <- median(loan.sub$tot_cur_bal, na.rm = T)
loan.sub$total_acc[which(is.na(loan.sub$total_acc))] <- median(loan.sub$total_acc, na.rm = T)
#loan.sub$open_acc[which(is.na(loan.sub$open_acc))] <- median(loan.sub$open_acc, na.rm = T)
loan.sub$log_annual_inc[which(is.na(loan.sub$log_annual_inc))] <- median(loan.sub$log_annual_inc, na.rm = T)

# Build model and evaluate performance
# split data into train and test for model performance
set.seed(1)
train.ind = sample(1:dim(loan)[1], 0.7*dim(loan)[1])
train = loan.sub[train.ind,]
test = loan.sub[-train.ind,]

mod = lm(int_rate ~ ., data = train)
summary(mod)
# If seeing NA in coefficient, it means almost perfect correlation between features, use alias(mod)


# See data points with negative fitted value, we should not predict negative interest rate
summary(train$int_rate)
summary(mod$fitted.values)

## transform int_rate into range -infinite to infinte
mod.2 = lm(log(int_rate) ~., data = train)
summary(mod.2)
# fitted or predicted interest rate
summary(train$int_rate)
summary(exp(mod.2$fitted.values))
summary(mod.2$residuals)
plot(exp(mod.2$fit), train$int_rate - exp(mod.2$fit), xlab = 'Fitted', ylab = 'residual')
# predict values on the test dataset
summary(test$int_rate)
pred.test <- round(exp(predict(mod.2, test)), 2)
# RMSE for training dataset
sqrt(sum(mod.2$residuals^2)/(dim(train)[1]))
# RMSE for test dataset
sqrt(sum((test$int_rate-pred.test)^2)/(dim(test)[1]))



# # still large residuals for some data points. Check the reason.
# pred <- round(exp(predict(mod.2, train)), 2)
# ## look these outliers
# ind = which(mod.2$fitted.values < log(4.5))
# length(ind)
# 
# cbind(train[ind,], pred=pred[ind], 
#       res=train[ind,'int_rate'] - pred[ind])
# ## the reason is: annual income is too large
# ## usually dti should not > 1
# train$log_annual_inc = log(train.sub$annual_inc)
# train$annual_inc = NULL
# mod.3 = lm(log(int_rate) ~., data = train)
# summary(exp(mod.3$fitted.values))

# first plot we can check unbiased/biased and homo/hetero of the residual
# Def not having homo, reason is model miss important features.
# second plot to check the normality of the residual. 
# qqplot: for ith percentile data point, find ith percentile in normal distribution.






# glmnet only takes matrix, can use is.data.frame() or is.matrix() to test
# glmnet standardizes every feature, even categorical feature
# http://stackoverflow.com/questions/17887747/how-does-glmnets-standardize-argument-handle-dummy-variables
library(glmnet)

# Scale data does not change results, only change betas.
## However, because of the magnificantion, we should standerize before reglarization
train.scale = train
train.scale[,c('log_annual_inc', 'dti', 'loan_amnt', 'total_acc', 'tot_cur_bal',  'installment')] = 
  scale(train.scale[,c('log_annual_inc', 'dti', 'loan_amnt', 'total_acc', 'tot_cur_bal', 'installment')])


ind = train.scale[,-1]
ind = model.matrix(~., ind)
dep = train.scale[,1]
fit = glmnet(x=ind, y=dep)
plot(fit, label = T)
plot(fit, xvar = 'lambda', label = T)

par(mfrow = c(1,2))
# Understand the plot
# The top row indicates the number of nonzero coefficients at the current Î»,
# which is the effective degrees of freedom (df) for the lasso.
# y axis is the value of coefficient
# x axis is the sum of absolute value of coefficients (L1 norm), or log(lambda)

par(mfrow = c(1,1))
par(mar = c(5.1, 6, 4.1, 2.1)) # default is par(mar=c(5.1,4.1,4.1,2.1), bottom, lef, top, right
vnat = coef(fit) # why do we see two intercepts, one is from model.matrix, one is default added in glmnet
vnat = vnat[-c(1,2), ncol(vnat)] # remove the intercept, and get the coefficients at the end of the path
plot(fit, xvar = 'lambda', label = T, yaxt = 'n', ylab = '')
axis(2, at=vnat,line=-.5,label = colnames(ind)[-1],las=1, cex.axis=0.5)


print(fit)
# Df is the non zero beta, 
# saturated model is a model with a parameter for every observation so that the data are fitted exactly.
# Deviance_model = 2*(loglikelihood_saturate_model - loglikelihood_current_model)
# Deviance_null = 2*(loglikelihood_saturate_model - loglikelihood_intercept_only_model)
# Deviance percentage = 1 -  Deviance_model / Deviance_null
# lambda value


# We can choose lambda by checking the picture, still kinda subjective
# use cross validation to get optimal value of lambda
cvfit = cv.glmnet(ind, dep)
plot(cvfit)
coef(cvfit)
coef(cvfit, s = "lambda.1se") # s stands for lambda
coef(cvfit, s = "lambda.min")


test.scale = test
test.scale[,c('log_annual_inc', 'dti', 'loan_amnt', 'total_acc', 'tot_cur_bal', 'installment')] = 
  scale(test.scale[,c('log_annual_inc', 'dti', 'loan_amnt', 'total_acc', 'tot_cur_bal', 'installment')])

ind.test = test.scale[,-1]
ind.test = model.matrix(~., ind.test)
preds = predict(cvfit, newx=ind.test, s="lambda.min")
sqrt(sum((test.scale$int_rate-preds)^2)/dim(test.scale)[1])


```